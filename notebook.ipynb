{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SalesDataBI\") \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Dataset from kaggle\n",
    "\n",
    "Data holds the basic information about sales data. The company have sales agencies / resellers and branches and the data file holds only the branch/reseller information in the customer field.\n",
    "\n",
    "Notes:\n",
    "Please note that, for the privacy issues, the customerID, SKU and documentID is processed with LabelEncoder. So, each customer or each product has a unique ID in the data file.\n",
    "\n",
    "Note 2: Data holds for more than 3 years, prices of the products can get raised by the time.\n",
    "\n",
    "DocumentID : ID of the transaction. A transaction might hold multiple records for the same customer at the same date with multiple products (SKU). DocumentID might be useful for combining the transactions and detecting the items sold together.\n",
    "Date : Date of transaction / sell. In the date time format.\n",
    "SKU : Item / Product code. The unique code for each item sold.\n",
    "Price : Sales price for the transaction. The price for the product for the customer for the date.\n",
    "Discount : discount amount for the transaction.\n",
    "Customer : Unique customer id for each customer. For the data set, customer can be a reseller or a branch of the company.\n",
    "Quantity : Number of items sold in the transaction.\n",
    "Version 2 Updates:\n",
    "Newer version of the data set has more rows until 2023 January. Also some of the column names are updated for the naming standards in other Kaggle notebooks.\n",
    "\n",
    "InvoiceID : ID of the transaction. A transaction might hold multiple records for the same customer at the same date with multiple products (SKU). DocumentID might be useful for combining the transactions and detecting the items sold together.\n",
    "Date : Date of transaction / sell. In the date time format.\n",
    "ProductID : Item / Product code. The unique code for each item sold.\n",
    "TotalSales : Sales price for the transaction. If you want to get unit_price , divide TotalSales column to Quantity column\n",
    "Discount : Discount amount for the transaction.\n",
    "CustomerID : Unique customer id for each customer. For the data set, customer can be a reseller or a branch of the company.\n",
    "Quantity : Number of items sold in the transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df1 = spark.read.option(\"header\", \"true\").csv(\"dataset/sales1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+--------+--------+--------+\n",
      "|_c0|DocumentID|      Date| SKU| Price|Discount|Customer|Quantity|\n",
      "+---+----------+----------+----+------+--------+--------+--------+\n",
      "|  0|       716|2019-09-23|1039|381.78|67.37254|       1|     1.0|\n",
      "|  1|       716|2019-09-23| 853|593.22| 0.00034|       1|     1.0|\n",
      "|  2|       716|2019-09-23| 862|423.73|-0.00119|       1|     1.0|\n",
      "|  3|       716|2019-09-23| 868| 201.7|35.58814|       1|     1.0|\n",
      "|  4|       716|2019-09-23|2313|345.76|61.01966|       1|     1.0|\n",
      "+---+----------+----------+----+------+--------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/06 03:58:45 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , DocumentID, Date, SKU, Price, Discount, Customer, Quantity\n",
      " Schema: _c0, DocumentID, Date, SKU, Price, Discount, Customer, Quantity\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ayoub/BI2Bigdata/dataset/sales1.csv\n"
     ]
    }
   ],
   "source": [
    "sales_df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df1 = sales_df1.withColumnRenamed(\"DocumentID\", \"InvoiceID\") \\\n",
    "                   .withColumnRenamed(\"SKU\", \"ProductID\") \\\n",
    "                   .withColumnRenamed(\"Price\", \"TotalSales\") \\\n",
    "                   .withColumnRenamed(\"Customer\", \"CustomerID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+---------+----------+--------+----------+--------+\n",
      "|_c0|InvoiceID|      Date|ProductID|TotalSales|Discount|CustomerID|Quantity|\n",
      "+---+---------+----------+---------+----------+--------+----------+--------+\n",
      "|  0|      716|2019-09-23|     1039|    381.78|67.37254|         1|     1.0|\n",
      "|  1|      716|2019-09-23|      853|    593.22| 0.00034|         1|     1.0|\n",
      "|  2|      716|2019-09-23|      862|    423.73|-0.00119|         1|     1.0|\n",
      "|  3|      716|2019-09-23|      868|     201.7|35.58814|         1|     1.0|\n",
      "|  4|      716|2019-09-23|     2313|    345.76|61.01966|         1|     1.0|\n",
      "+---+---------+----------+---------+----------+--------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/06 03:58:51 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , DocumentID, Date, SKU, Price, Discount, Customer, Quantity\n",
      " Schema: _c0, DocumentID, Date, SKU, Price, Discount, Customer, Quantity\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ayoub/BI2Bigdata/dataset/sales1.csv\n"
     ]
    }
   ],
   "source": [
    "sales_df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df2 = spark.read.option(\"header\", \"true\").csv(\"dataset/sales2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+---------+----------+--------+----------+--------+\n",
      "|_c0|InvoiceID|      Date|ProductID|TotalSales|Discount|CustomerID|Quantity|\n",
      "+---+---------+----------+---------+----------+--------+----------+--------+\n",
      "|  0|      716|2019-09-23|     1039|    381.78|67.37254|         1|     1.0|\n",
      "|  1|      716|2019-09-23|      853|    593.22| 0.00034|         1|     1.0|\n",
      "|  2|      716|2019-09-23|      862|    423.73|-0.00119|         1|     1.0|\n",
      "|  3|      716|2019-09-23|      868|     201.7|35.58814|         1|     1.0|\n",
      "|  4|      716|2019-09-23|     2313|    345.76|61.01966|         1|     1.0|\n",
      "+---+---------+----------+---------+----------+--------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/06 03:59:58 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , DocumentID, Date, SKU, Price, Discount, Customer, Quantity\n",
      " Schema: _c0, DocumentID, Date, SKU, Price, Discount, Customer, Quantity\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ayoub/BI2Bigdata/dataset/sales1.csv\n"
     ]
    }
   ],
   "source": [
    "sales_df = sales_df1.union(sales_df2)\n",
    "sales_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = sales_df.drop(\"_c0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales_df.write.format(\"delta\").mode(\"overwrite\").save(\"delta/sales_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slowly Changing Dimensions (SCD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer Dimension Table (SCD Type 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data = sales_df.select(\"CustomerID\", \"CustomerID\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "customer_dim = DeltaTable.forPath(spark, \"/path_to_delta/customer_dimension\")\n",
    "\n",
    "\n",
    "new_customer_data = spark.read.option(\"header\", \"true\").csv(\"path_to_new_customer_data.csv\")\n",
    "\n",
    "# Apply SCD Type 2: Update existing records with new customer data\n",
    "customer_dim.alias(\"old\") \\\n",
    "    .merge(new_customer_data.alias(\"new\"), \"old.CustomerID = new.CustomerID\") \\\n",
    "    .whenMatchedUpdate(\n",
    "        condition=\"old.Customer != new.Customer\",\n",
    "        set={\"valid_to\": F.current_date(), \"is_active\": F.lit(False)}  # Expire old record\n",
    "    ) \\\n",
    "    .whenNotMatchedInsert(\n",
    "        values={\n",
    "            \"CustomerID\": \"new.CustomerID\",\n",
    "            \"Customer\": \"new.Customer\",\n",
    "            \"valid_from\": F.current_date(),\n",
    "            \"is_active\": F.lit(True)\n",
    "        }\n",
    "    ) \\\n",
    "    .execute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
