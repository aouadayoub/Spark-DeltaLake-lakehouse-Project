{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SalesDataBI\") \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Dataset from kaggle\n",
    "\n",
    "Data holds the basic information about sales data. The company have sales agencies / resellers and branches and the data file holds only the branch/reseller information in the customer field.\n",
    "\n",
    "Notes:\n",
    "Please note that, for the privacy issues, the customerID, SKU and documentID is processed with LabelEncoder. So, each customer or each product has a unique ID in the data file.\n",
    "\n",
    "Note 2: Data holds for more than 3 years, prices of the products can get raised by the time.\n",
    "\n",
    "DocumentID : ID of the transaction. A transaction might hold multiple records for the same customer at the same date with multiple products (SKU). DocumentID might be useful for combining the transactions and detecting the items sold together.\n",
    "Date : Date of transaction / sell. In the date time format.\n",
    "SKU : Item / Product code. The unique code for each item sold.\n",
    "Price : Sales price for the transaction. The price for the product for the customer for the date.\n",
    "Discount : discount amount for the transaction.\n",
    "Customer : Unique customer id for each customer. For the data set, customer can be a reseller or a branch of the company.\n",
    "Quantity : Number of items sold in the transaction.\n",
    "Version 2 Updates:\n",
    "Newer version of the data set has more rows until 2023 January. Also some of the column names are updated for the naming standards in other Kaggle notebooks.\n",
    "\n",
    "InvoiceID : ID of the transaction. A transaction might hold multiple records for the same customer at the same date with multiple products (SKU). DocumentID might be useful for combining the transactions and detecting the items sold together.\n",
    "Date : Date of transaction / sell. In the date time format.\n",
    "ProductID : Item / Product code. The unique code for each item sold.\n",
    "TotalSales : Sales price for the transaction. If you want to get unit_price , divide TotalSales column to Quantity column\n",
    "Discount : Discount amount for the transaction.\n",
    "CustomerID : Unique customer id for each customer. For the data set, customer can be a reseller or a branch of the company.\n",
    "Quantity : Number of items sold in the transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df1 = spark.read.option(\"header\", \"true\").csv(\"dataset/sales1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+--------+--------+--------+\n",
      "|_c0|DocumentID|      Date| SKU| Price|Discount|Customer|Quantity|\n",
      "+---+----------+----------+----+------+--------+--------+--------+\n",
      "|  0|       716|2019-09-23|1039|381.78|67.37254|       1|     1.0|\n",
      "|  1|       716|2019-09-23| 853|593.22| 0.00034|       1|     1.0|\n",
      "|  2|       716|2019-09-23| 862|423.73|-0.00119|       1|     1.0|\n",
      "|  3|       716|2019-09-23| 868| 201.7|35.58814|       1|     1.0|\n",
      "|  4|       716|2019-09-23|2313|345.76|61.01966|       1|     1.0|\n",
      "+---+----------+----------+----+------+--------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/06 03:58:45 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , DocumentID, Date, SKU, Price, Discount, Customer, Quantity\n",
      " Schema: _c0, DocumentID, Date, SKU, Price, Discount, Customer, Quantity\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ayoub/BI2Bigdata/dataset/sales1.csv\n"
     ]
    }
   ],
   "source": [
    "sales_df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df1 = sales_df1.withColumnRenamed(\"DocumentID\", \"InvoiceID\") \\\n",
    "                   .withColumnRenamed(\"SKU\", \"ProductID\") \\\n",
    "                   .withColumnRenamed(\"Price\", \"TotalSales\") \\\n",
    "                   .withColumnRenamed(\"Customer\", \"CustomerID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+---------+----------+--------+----------+--------+\n",
      "|_c0|InvoiceID|      Date|ProductID|TotalSales|Discount|CustomerID|Quantity|\n",
      "+---+---------+----------+---------+----------+--------+----------+--------+\n",
      "|  0|      716|2019-09-23|     1039|    381.78|67.37254|         1|     1.0|\n",
      "|  1|      716|2019-09-23|      853|    593.22| 0.00034|         1|     1.0|\n",
      "|  2|      716|2019-09-23|      862|    423.73|-0.00119|         1|     1.0|\n",
      "|  3|      716|2019-09-23|      868|     201.7|35.58814|         1|     1.0|\n",
      "|  4|      716|2019-09-23|     2313|    345.76|61.01966|         1|     1.0|\n",
      "+---+---------+----------+---------+----------+--------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/06 03:58:51 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , DocumentID, Date, SKU, Price, Discount, Customer, Quantity\n",
      " Schema: _c0, DocumentID, Date, SKU, Price, Discount, Customer, Quantity\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ayoub/BI2Bigdata/dataset/sales1.csv\n"
     ]
    }
   ],
   "source": [
    "sales_df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df2 = spark.read.option(\"header\", \"true\").csv(\"dataset/sales2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+---------+----------+--------+----------+--------+\n",
      "|_c0|InvoiceID|      Date|ProductID|TotalSales|Discount|CustomerID|Quantity|\n",
      "+---+---------+----------+---------+----------+--------+----------+--------+\n",
      "|  0|      716|2019-09-23|     1039|    381.78|67.37254|         1|     1.0|\n",
      "|  1|      716|2019-09-23|      853|    593.22| 0.00034|         1|     1.0|\n",
      "|  2|      716|2019-09-23|      862|    423.73|-0.00119|         1|     1.0|\n",
      "|  3|      716|2019-09-23|      868|     201.7|35.58814|         1|     1.0|\n",
      "|  4|      716|2019-09-23|     2313|    345.76|61.01966|         1|     1.0|\n",
      "+---+---------+----------+---------+----------+--------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/06 03:59:58 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , DocumentID, Date, SKU, Price, Discount, Customer, Quantity\n",
      " Schema: _c0, DocumentID, Date, SKU, Price, Discount, Customer, Quantity\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/ayoub/BI2Bigdata/dataset/sales1.csv\n"
     ]
    }
   ],
   "source": [
    "sales_df = sales_df1.union(sales_df2)\n",
    "sales_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = sales_df.drop(\"_c0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales_df.write.format(\"delta\").mode(\"overwrite\").save(\"delta/sales_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slowly Changing Dimensions (SCD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer Dimension Table (SCD Type 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "`delta/customer_dimension` is not a Delta table.;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functions \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the customer dimension table (now that it exists)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m customer_dim \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta/customer_dimension\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load the new customer data (simulating with sales data)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m customer_data \u001b[38;5;241m=\u001b[39m sales_df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomerID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdistinct()\n",
      "File \u001b[0;32m~/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/readwriter.py:178\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, basestring):\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[0;32m~/spark/spark-3.0.3-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/utils.py:134\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    130\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: `delta/customer_dimension` is not a Delta table.;"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Load the customer dimension table (now that it exists)\n",
    "customer_dim = spark.read.format(\"delta\").load(\"delta/customer_dimension\")\n",
    "\n",
    "# Load the new customer data (simulating with sales data)\n",
    "customer_data = sales_df.select(\"CustomerID\", \"Customer\").distinct()\n",
    "\n",
    "# Merge operation for SCD Type 2: Update existing records and insert new ones\n",
    "customer_data.alias(\"new\") \\\n",
    "    .join(customer_dim.alias(\"old\"), \"new.CustomerID = old.CustomerID\", \"left_outer\") \\\n",
    "    .withColumn(\"is_match\", F.when(F.col(\"old.CustomerID\").isNull(), F.lit(False)).otherwise(F.lit(True))) \\\n",
    "    .withColumn(\"valid_to\", F.when(F.col(\"is_match\") == False, F.current_date()).otherwise(F.lit(None))) \\\n",
    "    .withColumn(\"is_active\", F.when(F.col(\"is_match\") == False, F.lit(True)).otherwise(F.lit(False))) \\\n",
    "    .withColumn(\"valid_from\", F.when(F.col(\"is_match\") == False, F.current_date()).otherwise(F.lit(None))) \\\n",
    "    .select(\"new.CustomerID\", \"new.Customer\", \"valid_from\", \"valid_to\", \"is_active\") \\\n",
    "    .createOrReplaceTempView(\"customer_updates\")\n",
    "\n",
    "\n",
    "customer_dim_df = spark.read.format(\"delta\").load(\"delta/customer_dimension\")\n",
    "\n",
    "customer_dim_df.createOrReplaceTempView(\"customer_dim_table\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO customer_dim_table AS old\n",
    "USING customer_updates AS new\n",
    "ON old.CustomerID = new.CustomerID\n",
    "WHEN MATCHED AND old.Customer != new.Customer AND old.is_active = TRUE\n",
    "    THEN UPDATE SET old.valid_to = current_date(), old.is_active = FALSE\n",
    "WHEN NOT MATCHED\n",
    "    THEN INSERT (CustomerID, Customer, valid_from, valid_to, is_active)\n",
    "         VALUES (new.CustomerID, new.Customer, new.valid_from, new.valid_to, new.is_active)\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
